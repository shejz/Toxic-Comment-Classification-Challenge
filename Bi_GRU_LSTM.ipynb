{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bi-GRU-LSTM.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUP4Ijb7UbnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kai3rEQnUloH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f4a45d64-406a-4200-bb16-1bc0b18d4ce7"
      },
      "source": [
        "print(os.listdir(\"/content\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.config', 'test.csv', 'train.csv', 'test_labels.csv', '.ipynb_checkpoints', 'kaggle.json', 'glove.twitter.27B.200d.txt', 'crawl-300d-2M.vec', 'sample_submission.csv', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9z56sKQVCtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace all numeric with 'n'\n",
        "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
        "\n",
        "COMMENT_COL = 'comment_text'\n",
        "ID_COL = 'id'\n",
        "input_dir = '/content/'\n",
        "output_dir = '/content/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYORTME3VK8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# redundancy words and their right formats\n",
        "redundancy_rightFormat = {\n",
        "    'ckckck': 'cock',\n",
        "    'fuckfuck': 'fuck',\n",
        "    'lolol': 'lol',\n",
        "    'lollol': 'lol',\n",
        "    'pussyfuck':'fuck',\n",
        "    'gaygay': 'gay',\n",
        "    'haha': 'ha',\n",
        "    'sucksuck': 'suck'}\n",
        "\n",
        "redundancy = set(redundancy_rightFormat.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjFwL07mVLja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all the words below are included in glove dictionary\n",
        "# combine these toxic indicators with 'CommProcess.revise_triple_and_more_letters'\n",
        "toxic_indicator_words = [\n",
        "    'fuck', 'fucking', 'fucked', 'fuckin', 'fucka', 'fucker', 'fucks', 'fuckers',\n",
        "    'fck', 'fcking', 'fcked', 'fckin', 'fcker', 'fcks',\n",
        "    'fuk', 'fuking', 'fuked', 'fukin', 'fuker', 'fuks', 'fukers',\n",
        "    'fk', 'fking', 'fked', 'fkin', 'fker', 'fks',\n",
        "    'shit', 'shitty', 'shite',\n",
        "    'stupid', 'stupids',\n",
        "    'idiot', 'idiots',\n",
        "    'suck', 'sucker', 'sucks', 'sucka', 'sucked', 'sucking',\n",
        "    'ass', 'asses', 'asshole', 'assholes', 'ashole', 'asholes',\n",
        "    'gay', 'gays',\n",
        "    'niga', 'nigga', 'nigar', 'niggar', 'niger', 'nigger',\n",
        "    'monster', 'monsters',\n",
        "    'loser', 'losers',\n",
        "    'nazi', 'nazis',\n",
        "    'cock', 'cocks', 'cocker', 'cockers',\n",
        "    'faggot', 'faggy',\n",
        "]\n",
        "toxic_indicator_words_sets = set(toxic_indicator_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTAZTuzOVOmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _get_toxicIndicator_transformers():\n",
        "    toxicIndicator_transformers = dict()\n",
        "    for word in toxic_indicator_words:\n",
        "        tmp_1 = []\n",
        "        for c in word:\n",
        "            if len(tmp_1) > 0:\n",
        "                tmp_2 = []\n",
        "                for pre in tmp_1:\n",
        "                    tmp_2.append(pre + c)\n",
        "                    tmp_2.append(pre + c + c)\n",
        "                tmp_1 = tmp_2\n",
        "            else:\n",
        "                tmp_1.append(c)\n",
        "                tmp_1.append(c + c)\n",
        "        toxicIndicator_transformers[word] = tmp_1\n",
        "    return toxicIndicator_transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrExq_k2VRLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toxicIndicator_transformers = _get_toxicIndicator_transformers()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STg-Jr9TVRv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deny_origin = {\n",
        "    \"you're\": ['you', 'are'],\n",
        "    \"i'm\": ['i', 'am'],\n",
        "    \"he's\": ['he', 'is'],\n",
        "    \"she's\": ['she', 'is'],\n",
        "    \"it's\": ['it', 'is'],\n",
        "    \"they're\": ['they', 'are'],\n",
        "    \"can't\": ['can', 'not'],\n",
        "    \"couldn't\": ['could', 'not'],\n",
        "    \"don't\": ['do', 'not'],\n",
        "    \"don;t\": ['do', 'not'],\n",
        "    \"didn't\": ['did', 'not'],\n",
        "    \"doesn't\": ['does', 'not'],\n",
        "    \"isn't\": ['is', 'not'],\n",
        "    \"wasn't\": ['was', 'not'],\n",
        "    \"aren't\": ['are', 'not'],\n",
        "    \"weren't\": ['were', 'not'],\n",
        "    \"won't\": ['will', 'not'],\n",
        "    \"wouldn't\": ['would', 'not'],\n",
        "    \"hasn't\": ['has', 'not'],\n",
        "    \"haven't\": ['have', 'not'],\n",
        "    \"what's\": ['what', 'is'],\n",
        "    \"that's\": ['that', 'is'],\n",
        "}\n",
        "denies = set(deny_origin.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKNOAU-uVanU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CommProcess(object):\n",
        "    @staticmethod\n",
        "    def clean_text(t):\n",
        "        t = re.sub(r\"[^A-Za-z0-9,!?*.;’´'\\/]\", \" \", t)\n",
        "        t = replace_numbers.sub(\" \", t)\n",
        "        t = t.lower()\n",
        "        t = re.sub(r\",\", \" \", t)\n",
        "        t = re.sub(r\"’\", \"'\", t)\n",
        "        t = re.sub(r\"´\", \"'\", t)\n",
        "        t = re.sub(r\"\\.\", \" \", t)\n",
        "        t = re.sub(r\"!\", \" ! \", t)\n",
        "        t = re.sub(r\"\\?\", \" ? \", t)\n",
        "        t = re.sub(r\"\\/\", \" \", t)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_deny(t):\n",
        "        ret = []\n",
        "        for word in t.split():\n",
        "            if word in denies:\n",
        "                ret.append(deny_origin[word][0])\n",
        "                ret.append(deny_origin[word][1])\n",
        "            else:\n",
        "                ret.append(word)\n",
        "        ret = ' '.join(ret)\n",
        "        ret = re.sub(\"'\", \" \", ret)\n",
        "        ret = re.sub(r\";\", \" \", ret)\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_star(t):\n",
        "        ret = []\n",
        "        for word in t.split():\n",
        "            if ('*' in word) and (re.sub('\\*', '', word) in toxic_indicator_words_sets):\n",
        "                word = re.sub('\\*', '', word)\n",
        "            ret.append(word)\n",
        "        ret = re.sub('\\*', ' ', ' '.join(ret))\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_triple_and_more_letters(t):\n",
        "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "            reg = letter + \"{2,}\"\n",
        "            t = re.sub(reg, letter + letter, t)\n",
        "        return t\n",
        "\n",
        "    @staticmethod\n",
        "    def revise_redundancy_words(t):\n",
        "        ret = []\n",
        "        for word in t.split(' '):\n",
        "            for redu in redundancy:\n",
        "                if redu in word:\n",
        "                    word = redundancy_rightFormat[redu]\n",
        "                    break\n",
        "            ret.append(word)\n",
        "        return ' '.join(ret)\n",
        "\n",
        "    @staticmethod\n",
        "    def fill_na(t):\n",
        "        if t.strip() == '':\n",
        "            return 'NA'\n",
        "        return t\n",
        "\n",
        "\n",
        "def execute_comm_process(df):\n",
        "    comm_process_pipeline = [\n",
        "        CommProcess.clean_text,\n",
        "        CommProcess.revise_deny,\n",
        "        CommProcess.revise_star,\n",
        "        CommProcess.revise_triple_and_more_letters,\n",
        "        CommProcess.revise_redundancy_words,\n",
        "        CommProcess.fill_na,\n",
        "    ]\n",
        "    for cp in comm_process_pipeline:\n",
        "        df[COMMENT_COL] = df[COMMENT_COL].apply(cp)\n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsE1S_xhVrn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "088d7d42-6099-4ebf-ee6a-deef31e0cbb5"
      },
      "source": [
        "# Process whole train data\n",
        "print('Comm processing whole train data')\n",
        "df_train = pd.read_csv(input_dir + 'train.csv')\n",
        "df_train = execute_comm_process(df_train)\n",
        "df_train.to_csv('train_processed.csv', index=False)\n",
        "\n",
        "# Process test data\n",
        "print('Comm processing test data')\n",
        "df_test = pd.read_csv(input_dir + 'test.csv')\n",
        "df_test = execute_comm_process(df_test)\n",
        "df_test.to_csv('test_processed.csv', index=False)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Comm processing whole train data\n",
            "Comm processing test data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFLLNLIqXSPK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50954dbe-3820-46fd-ab2a-ae9c81b36349"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "import gensim\n",
        "from collections import Counter\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
        "from keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import CuDNNLSTM, CuDNNGRU\n",
        "from keras.preprocessing import text, sequence\n",
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import optimizers\n",
        "from keras.layers import Lambda\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "\n",
        "import gc\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from unidecode import unidecode\n",
        "\n",
        "import time\n",
        "\n",
        "eng_stopwords = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnTgTovzXuLM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. preprocessing\n",
        "train = pd.read_csv(\"train_processed.csv\")\n",
        "test = pd.read_csv(\"test_processed.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcIDEUdNX9_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2.  remove non-ascii\n",
        "\n",
        "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n",
        "def clean_text(x):\n",
        "    x_ascii = unidecode(x)\n",
        "    x_clean = special_character_removal.sub('',x_ascii)\n",
        "    return x_clean\n",
        "\n",
        "train['clean_text'] = train['comment_text'].apply(lambda x: clean_text(str(x)))\n",
        "test['clean_text'] = test['comment_text'].apply(lambda x: clean_text(str(x)))\n",
        "\n",
        "X_train = train['clean_text'].fillna(\"something\").values\n",
        "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
        "X_test = test['clean_text'].fillna(\"something\").values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQEogrFoYC0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_features(df):\n",
        "    \n",
        "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
        "    df['total_length'] = df['comment_text'].apply(len)\n",
        "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
        "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
        "                                axis=1)\n",
        "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
        "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
        "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
        "\n",
        "    return df\n",
        "\n",
        "train = add_features(train)\n",
        "test = add_features(test)\n",
        "\n",
        "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
        "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
        "\n",
        "ss = StandardScaler()\n",
        "ss.fit(np.vstack((features, test_features)))\n",
        "features = ss.transform(features)\n",
        "test_features = ss.transform(test_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf6oPZVDYNAs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e01a7bd-fea6-4401-f26a-2c7b44b6c27a"
      },
      "source": [
        " max_features = 283759\n",
        " maxlen = 900\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
        "X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "x_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\n",
        "print(len(tokenizer.word_index))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "283759\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muqh72FDZRgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the FastText Web Crawl vectors\n",
        "EMBEDDING_FILE_FASTTEXT = \"crawl-300d-2M.vec\"\n",
        "EMBEDDING_FILE_TWITTER = \"glove.twitter.27B.200d.txt\"\n",
        "\n",
        "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
        "embeddings_index_ft = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\n",
        "embeddings_index_tw = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzfI_XFjcPl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP6R2Do2cXsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This code is  based on: Spellchecker using Word2vec \n",
        "words = spell_model.index2word\n",
        "\n",
        "w_rank = {}\n",
        "for i,word in enumerate(words):\n",
        "    w_rank[word] = i\n",
        "\n",
        "WORDS = w_rank\n",
        "\n",
        "# Use fast text as vocabulary\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def P(word): \n",
        "    \"Probability of `word`.\"\n",
        "    # use inverse of rank as proxy\n",
        "    # returns 0 if the word isn't in the dictionary\n",
        "    return - WORDS.get(word, 0)\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "def singlify(word):\n",
        "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI0WqQuKcdqn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "bfea5a4f-5085-4c9a-f05c-c77d5e298692"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words,501))\n",
        "\n",
        "something_tw = embeddings_index_tw.get(\"something\")\n",
        "something_ft = embeddings_index_ft.get(\"something\")\n",
        "\n",
        "something = np.zeros((501,))\n",
        "something[:300,] = something_ft\n",
        "something[300:500,] = something_tw\n",
        "something[500,] = 0\n",
        "\n",
        "def all_caps(word):\n",
        "    return len(word) > 1 and word.isupper()\n",
        "\n",
        "def embed_word(embedding_matrix,i,word):\n",
        "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
        "    if embedding_vector_ft is not None: \n",
        "        if all_caps(word):\n",
        "            last_value = np.array([1])\n",
        "        else:\n",
        "            last_value = np.array([0])\n",
        "        embedding_matrix[i,:300] = embedding_vector_ft\n",
        "        embedding_matrix[i,500] = last_value\n",
        "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
        "        if embedding_vector_tw is not None:\n",
        "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
        "\n",
        "            \n",
        "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
        "for word, i in word_index.items():\n",
        "    \n",
        "    if i >= max_features: continue\n",
        "        \n",
        "    if embeddings_index_ft.get(word) is not None:\n",
        "        embed_word(embedding_matrix,i,word)\n",
        "    else:\n",
        "        # change to > 20 for better score.\n",
        "        if len(word) > 25:\n",
        "            embedding_matrix[i] = something\n",
        "        else:\n",
        "            word2 = correction(word)\n",
        "            if embeddings_index_ft.get(word2) is not None:\n",
        "                embed_word(embedding_matrix,i,word2)\n",
        "            else:\n",
        "                word2 = correction(singlify(word))\n",
        "                if embeddings_index_ft.get(word2) is not None:\n",
        "                    embed_word(embedding_matrix,i,word2)\n",
        "                else:\n",
        "                    embedding_matrix[i] = something     "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-36c0714fccf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0membed_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mword2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msinglify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0membeddings_index_ft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0membed_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6565a3957fef>\u001b[0m in \u001b[0;36mcorrection\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;34m\"Most probable spelling correction for word.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6565a3957fef>\u001b[0m in \u001b[0;36mcandidates\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"Generate possible spelling corrections for word.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medits2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6565a3957fef>\u001b[0m in \u001b[0;36mknown\u001b[0;34m(words)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m\"The subset of `words` that appear in the dictionary of WORDS.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-6565a3957fef>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mknown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m\"The subset of `words` that appear in the dictionary of WORDS.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0medits1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8sz6FoNc6Q-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "        self.max_score = 0\n",
        "        self.not_better_count = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
        "            score = roc_auc_score(self.y_val, y_pred)\n",
        "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
        "            if (score > self.max_score):\n",
        "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
        "                model.save_weights(\"best_weights.h5\")\n",
        "                self.max_score=score\n",
        "                self.not_better_count = 0\n",
        "            else:\n",
        "                self.not_better_count += 1\n",
        "                if self.not_better_count > 3:\n",
        "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
        "                    self.model.stop_training = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SV84cW9c6_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(features,clipvalue=1.,num_filters=40,dropout=0.5,embed_size=501):\n",
        "    features_input = Input(shape=(features.shape[1],))\n",
        "    inp = Input(shape=(maxlen, ))\n",
        "    \n",
        "    # Layer 1: concatenated fasttext and glove twitter embeddings.\n",
        "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    \n",
        "    # Uncomment for best result\n",
        "    # Layer 2: SpatialDropout1D(0.5)\n",
        "    x = SpatialDropout1D(dropout)(x)\n",
        "    \n",
        "    # Uncomment for best result\n",
        "    # Layer 3: Bidirectional CuDNNLSTM\n",
        "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
        "\n",
        "\n",
        "    # Layer 4: Bidirectional CuDNNGRU\n",
        "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n",
        "    \n",
        "    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n",
        "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
        "    avg_pool = GlobalAveragePooling1D()(x)\n",
        "    max_pool = GlobalMaxPooling1D()(x)\n",
        "    \n",
        "    x = concatenate([avg_pool, x_h, max_pool,features_input])\n",
        "    \n",
        "    # Layer 6: output dense layer.\n",
        "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=[inp,features_input], outputs=outp)\n",
        "    adam = optimizers.adam(clipvalue=clipvalue)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer=adam,\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_mrb7wxdEFu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "outputId": "34ab2a47-9acb-4dc8-83f8-39c6cbcadae1"
      },
      "source": [
        "model = get_model(features)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Used epochs=100 with early exiting for best score.\n",
        "epochs = 10\n",
        "gc.collect()\n",
        "K.clear_session()\n",
        "\n",
        "# Change to 10\n",
        "num_folds = 10 #number of folds\n",
        "\n",
        "predict = np.zeros((test.shape[0],6))\n",
        "\n",
        "# Uncomment for out-of-fold predictions\n",
        "#scores = []\n",
        "#oof_predict = np.zeros((train.shape[0],6))\n",
        "\n",
        "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
        "\n",
        "for train_index, test_index in kf.split(x_train):\n",
        "    \n",
        "    kfold_y_train,kfold_y_test = y_train[train_index], y_train[test_index]\n",
        "    kfold_X_train = x_train[train_index]\n",
        "    kfold_X_features = features[train_index]\n",
        "    kfold_X_valid = x_train[test_index]\n",
        "    kfold_X_valid_features = features[test_index] \n",
        "    \n",
        "    gc.collect()\n",
        "    K.clear_session()\n",
        "    \n",
        "    model = get_model(features)\n",
        "    \n",
        "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n",
        "    \n",
        "    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "             callbacks = [ra_val])\n",
        "    gc.collect()\n",
        "    \n",
        "    #model.load_weights(bst_model_path)\n",
        "    model.load_weights(\"best_weights.h5\")\n",
        "    \n",
        "    predict += model.predict([x_test,test_features], batch_size=batch_size,verbose=1) / num_folds\n",
        "    \n",
        "    #gc.collect()\n",
        "    # uncomment for out of fold predictions\n",
        "    #oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n",
        "    #cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n",
        "    \n",
        "    #scores.append(cv_score)\n",
        "    #print('score: ',cv_score)\n",
        "\n",
        "print(\"Done\")\n",
        "#print('Total CV score is {}'.format(np.mean(scores)))    \n",
        "\n",
        "\n",
        "sample_submission = pd.read_csv(\"/content/sample_submission.csv\")\n",
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "sample_submission[class_names] = predict\n",
        "sample_submission.to_csv('model_9872_baseline_submission.csv',index=False)\n",
        "\n",
        "# uncomment for out of fold predictions\n",
        "#oof = pd.DataFrame.from_dict({'id': train['id']})\n",
        "#for c in class_names:\n",
        "#    oof[c] = np.zeros(len(train))\n",
        "#    \n",
        "#oof[class_names] = oof_predict\n",
        "#for c in class_names:\n",
        "#    oof['prediction_' +c] = oof[c]\n",
        "#oof.to_csv('oof-model_9872_baseline_submission.csv', index=False)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "79785/79785 [==============================] - 2722s 34ms/step - loss: 0.0578 - accuracy: 0.9797\n",
            "79786/79786 [==============================] - 815s 10ms/step\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.982607 \n",
            "\n",
            "*** New High Score (previous: 0.000000) \n",
            "\n",
            "153164/153164 [==============================] - 1555s 10ms/step\n",
            "Epoch 1/1\n",
            "79786/79786 [==============================] - 2734s 34ms/step - loss: 0.0579 - accuracy: 0.9797\n",
            "79785/79785 [==============================] - 822s 10ms/step\n",
            "\n",
            " ROC-AUC - epoch: 1 - score: 0.983644 \n",
            "\n",
            "*** New High Score (previous: 0.000000) \n",
            "\n",
            "153164/153164 [==============================] - 1571s 10ms/step\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}