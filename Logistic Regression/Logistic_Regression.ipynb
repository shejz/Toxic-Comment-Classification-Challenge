{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRjdgEk0Vezj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yh_VTTCVg-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod(\n",
        "            (datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' %\n",
        "              (thour, tmin, round(tsec, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMPRyX-YWXRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_path = '/content/drive/My Drive/Toxic Comment Classification/'\n",
        "\n",
        "train = pd.read_csv(f'{folder_path}train.csv')\n",
        "test = pd.read_csv(f'{folder_path}test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWSM2V6TVk99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "\n",
        "traintime = timer(None)\n",
        "train_time = timer(None)\n",
        "tr_ids = train[['id']]\n",
        "train[class_names] = train[class_names].astype(np.int8)\n",
        "target = train[class_names]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNFblVJWkqG",
        "colab_type": "code",
        "outputId": "21d0ccaa-f2b5-4734-838b-8aff52863f06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(' Cleaning ...')\n",
        "# PREPROCESSING PART\n",
        "repl = {\n",
        "    \"yay!\": \" good \",\n",
        "    \"yay\": \" good \",\n",
        "    \"yaay\": \" good \",\n",
        "    \"yaaay\": \" good \",\n",
        "    \"yaaaay\": \" good \",\n",
        "    \"yaaaaay\": \" good \",\n",
        "    \":/\": \" bad \",\n",
        "    \":&gt;\": \" sad \",\n",
        "    \":')\": \" sad \",\n",
        "    \":-(\": \" frown \",\n",
        "    \":(\": \" frown \",\n",
        "    \":s\": \" frown \",\n",
        "    \":-s\": \" frown \",\n",
        "    \"&lt;3\": \" heart \",\n",
        "    \":d\": \" smile \",\n",
        "    \":p\": \" smile \",\n",
        "    \":dd\": \" smile \",\n",
        "    \"8)\": \" smile \",\n",
        "    \":-)\": \" smile \",\n",
        "    \":)\": \" smile \",\n",
        "    \";)\": \" smile \",\n",
        "    \"(-:\": \" smile \",\n",
        "    \"(:\": \" smile \",\n",
        "    \":/\": \" worry \",\n",
        "    \":&gt;\": \" angry \",\n",
        "    \":')\": \" sad \",\n",
        "    \":-(\": \" sad \",\n",
        "    \":(\": \" sad \",\n",
        "    \":s\": \" sad \",\n",
        "    \":-s\": \" sad \",\n",
        "    r\"\\br\\b\": \"are\",\n",
        "    r\"\\bu\\b\": \"you\",\n",
        "    r\"\\bhaha\\b\": \"ha\",\n",
        "    r\"\\bhahaha\\b\": \"ha\",\n",
        "    r\"\\bdon't\\b\": \"do not\",\n",
        "    r\"\\bdoesn't\\b\": \"does not\",\n",
        "    r\"\\bdidn't\\b\": \"did not\",\n",
        "    r\"\\bhasn't\\b\": \"has not\",\n",
        "    r\"\\bhaven't\\b\": \"have not\",\n",
        "    r\"\\bhadn't\\b\": \"had not\",\n",
        "    r\"\\bwon't\\b\": \"will not\",\n",
        "    r\"\\bwouldn't\\b\": \"would not\",\n",
        "    r\"\\bcan't\\b\": \"can not\",\n",
        "    r\"\\bcannot\\b\": \"can not\",\n",
        "    r\"\\bi'm\\b\": \"i am\",\n",
        "    \"m\": \"am\",\n",
        "    \"r\": \"are\",\n",
        "    \"u\": \"you\",\n",
        "    \"haha\": \"ha\",\n",
        "    \"hahaha\": \"ha\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"can't\": \"can not\",\n",
        "    \"cannot\": \"can not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"m\": \"am\",\n",
        "    \"i'll\" : \"i will\",\n",
        "    \"its\" : \"it is\",\n",
        "    \"it's\" : \"it is\",\n",
        "    \"'s\" : \" is\",\n",
        "    \"that's\" : \"that is\",\n",
        "    \"weren't\" : \"were not\",\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Cleaning ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ybDOhaWy7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = [i for i in repl.keys()]\n",
        "\n",
        "new_train_data = []\n",
        "new_test_data = []\n",
        "ltr = train[\"comment_text\"].tolist()\n",
        "lte = test[\"comment_text\"].tolist()\n",
        "for i in ltr:\n",
        "    arr = str(i).split()\n",
        "    xx = \"\"\n",
        "    for j in arr:\n",
        "        j = str(j).lower()\n",
        "        if j[:4] == 'http' or j[:3] == 'www':\n",
        "            continue\n",
        "        if j in keys:\n",
        "            j = repl[j]\n",
        "        xx += j + \" \"\n",
        "    new_train_data.append(xx)\n",
        "for i in lte:\n",
        "    arr = str(i).split()\n",
        "    xx = \"\"\n",
        "    for j in arr:\n",
        "        j = str(j).lower()\n",
        "        if j[:4] == 'http' or j[:3] == 'www':\n",
        "            continue\n",
        "        if j in keys:\n",
        "            j = repl[j]\n",
        "        xx += j + \" \"\n",
        "    new_test_data.append(xx)\n",
        "train[\"new_comment_text\"] = new_train_data\n",
        "test[\"new_comment_text\"] = new_test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPMHepWLXC0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trate = train[\"new_comment_text\"].tolist()\n",
        "tete = test[\"new_comment_text\"].tolist()\n",
        "for i, c in enumerate(trate):\n",
        "    trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n",
        "for i, c in enumerate(tete):\n",
        "    tete[i] = re.sub('[^a-zA-Z ?!]+', '', tete[i])\n",
        "train[\"comment_text\"] = trate\n",
        "test[\"comment_text\"] = tete\n",
        "del trate, tete\n",
        "train.drop([\"new_comment_text\"], axis=1, inplace=True)\n",
        "test.drop([\"new_comment_text\"], axis=1, inplace=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTLAwWPvYulD",
        "colab_type": "code",
        "outputId": "8d4eadd0-80d2-49b5-a1be-bca5f46097fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_text = train['comment_text']\n",
        "test_text = test['comment_text']\n",
        "all_text = pd.concat([train_text, test_text])\n",
        "timer(train_time)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Time taken: 0 hours 0 minutes and 34.62 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhtO6QI9Y1ID",
        "colab_type": "code",
        "outputId": "d0adfb9f-64d9-4582-861d-220c47792b9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_time = timer(None)\n",
        "print(' Part 1/2 of vectorizing ...')\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='word',\n",
        "    token_pattern=r'\\w{1,}',\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1),\n",
        "    max_features=10000)\n",
        "word_vectorizer.fit(all_text)\n",
        "train_word_features = word_vectorizer.transform(train_text)\n",
        "test_word_features = word_vectorizer.transform(test_text)\n",
        "timer(train_time)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Part 1/2 of vectorizing ...\n",
            "\n",
            " Time taken: 0 hours 0 minutes and 27.22 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiqLayrWY33V",
        "colab_type": "code",
        "outputId": "6617eed0-c9d4-4b8f-ef89-a4f9de033299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "train_time = timer(None)\n",
        "print(' Part 2/2 of vectorizing ...')\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    strip_accents='unicode',\n",
        "    analyzer='char',\n",
        "    stop_words='english',\n",
        "    ngram_range=(2, 6),\n",
        "    max_features=50000)\n",
        "char_vectorizer.fit(all_text)\n",
        "train_char_features = char_vectorizer.transform(train_text)\n",
        "test_char_features = char_vectorizer.transform(test_text)\n",
        "timer(train_time)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Part 2/2 of vectorizing ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Time taken: 0 hours 14 minutes and 27.23 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6LnRtGWZCFo",
        "colab_type": "code",
        "outputId": "b2f89dc2-2071-4c20-cd3c-ee396664bc10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_features = hstack([train_char_features, train_word_features]).tocsr()\n",
        "test_features = hstack([test_char_features, test_word_features]).tocsr()\n",
        "timer(traintime)\n",
        "\n",
        "all_parameters = {\n",
        "                  'C'             : [1.048113, 0.1930, 0.596362, 0.25595, 0.449843, 0.25595],\n",
        "                  'tol'           : [0.1, 0.1, 0.046416, 0.0215443, 0.1, 0.01],\n",
        "                  'solver'        : ['lbfgs', 'newton-cg', 'lbfgs', 'newton-cg', 'newton-cg', 'lbfgs'],\n",
        "                  'fit_intercept' : [True, True, True, True, True, True],\n",
        "                  'penalty'       : ['l2', 'l2', 'l2', 'l2', 'l2', 'l2'],\n",
        "                  'class_weight'  : [None, 'balanced', 'balanced', 'balanced', 'balanced', 'balanced'],\n",
        "                 }\n",
        "\n",
        "folds = 3\n",
        "scores = []\n",
        "scores_classes = np.zeros((len(class_names), folds))\n",
        "\n",
        "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
        "submission_oof = train[['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n",
        "kf = KFold(n_splits=folds, shuffle=True, random_state=239)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Time taken: 0 hours 15 minutes and 57.8 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-n6eDeyZLB1",
        "colab_type": "code",
        "outputId": "c1774b13-8e14-4ca8-dc1a-9ee275f2d6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "idpred = tr_ids\n",
        "\n",
        "traintime = timer(None)\n",
        "for j, (class_name) in enumerate(class_names):\n",
        "    classifier = LogisticRegression(\n",
        "        C=all_parameters['C'][j],\n",
        "        max_iter=200,\n",
        "        tol=all_parameters['tol'][j],\n",
        "        solver=all_parameters['solver'][j],\n",
        "        fit_intercept=all_parameters['fit_intercept'][j],\n",
        "        penalty=all_parameters['penalty'][j],\n",
        "        dual=False,\n",
        "        class_weight=all_parameters['class_weight'][j],\n",
        "        verbose=0)\n",
        "\n",
        "    avreal = target[class_name]\n",
        "    lr_cv_sum = 0\n",
        "    lr_pred = []\n",
        "    lr_fpred = []\n",
        "    lr_avpred = np.zeros(train.shape[0])\n",
        "\n",
        "    train_time = timer(None)\n",
        "    for i, (train_index, val_index) in enumerate(kf.split(train_features)):\n",
        "        X_train, X_val = train_features[train_index], train_features[val_index]\n",
        "        y_train, y_val = target.loc[train_index], target.loc[val_index]\n",
        "\n",
        "        classifier.fit(X_train, y_train[class_name])\n",
        "        scores_val = classifier.predict_proba(X_val)[:, 1]\n",
        "        lr_avpred[val_index] = scores_val\n",
        "        lr_y_pred = classifier.predict_proba(test_features)[:, 1]\n",
        "        scores_classes[j][i] = roc_auc_score(y_val[class_name], scores_val)\n",
        "        print('\\n Fold %02d class %s AUC: %.6f' % ((i+1), class_name, scores_classes[j][i]))\n",
        "\n",
        "        if i > 0:\n",
        "            lr_fpred = lr_pred + lr_y_pred\n",
        "        else:\n",
        "            lr_fpred = lr_y_pred\n",
        "\n",
        "        lr_pred = lr_fpred\n",
        "\n",
        "    lr_cv_score = (lr_cv_sum / folds)\n",
        "    lr_oof_auc = roc_auc_score(avreal, lr_avpred)\n",
        "    print('\\n Average class %s AUC:\\t%.6f' % (class_name, np.mean(scores_classes[j])))\n",
        "    print(' Out-of-fold class %s AUC:\\t%.6f' % (class_name, lr_oof_auc))\n",
        "    timer(train_time)\n",
        "\n",
        "    submission[class_name] = lr_pred / folds\n",
        "    submission_oof['prediction_' + class_name] = lr_avpred\n",
        "\n",
        "print('\\n Overall AUC:\\t%.6f' % (np.mean(scores_classes)))\n",
        "submission.to_csv('tuned-LR.csv', index=False)\n",
        "submission_oof.to_csv('oof-tuned-LR.csv', index=False)\n",
        "timer(traintime)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Fold 01 class toxic AUC: 0.978664\n",
            "\n",
            " Fold 02 class toxic AUC: 0.977361\n",
            "\n",
            " Fold 03 class toxic AUC: 0.977651\n",
            "\n",
            " Average class toxic AUC:\t0.977892\n",
            " Out-of-fold class toxic AUC:\t0.977888\n",
            "\n",
            " Time taken: 0 hours 3 minutes and 34.55 seconds.\n",
            "\n",
            " Fold 01 class severe_toxic AUC: 0.988105\n",
            "\n",
            " Fold 02 class severe_toxic AUC: 0.988785\n",
            "\n",
            " Fold 03 class severe_toxic AUC: 0.988934\n",
            "\n",
            " Average class severe_toxic AUC:\t0.988608\n",
            " Out-of-fold class severe_toxic AUC:\t0.988604\n",
            "\n",
            " Time taken: 0 hours 3 minutes and 33.39 seconds.\n",
            "\n",
            " Fold 01 class obscene AUC: 0.990228\n",
            "\n",
            " Fold 02 class obscene AUC: 0.990318\n",
            "\n",
            " Fold 03 class obscene AUC: 0.990632\n",
            "\n",
            " Average class obscene AUC:\t0.990393\n",
            " Out-of-fold class obscene AUC:\t0.990391\n",
            "\n",
            " Time taken: 0 hours 2 minutes and 28.67 seconds.\n",
            "\n",
            " Fold 01 class threat AUC: 0.989392\n",
            "\n",
            " Fold 02 class threat AUC: 0.990747\n",
            "\n",
            " Fold 03 class threat AUC: 0.991784\n",
            "\n",
            " Average class threat AUC:\t0.990641\n",
            " Out-of-fold class threat AUC:\t0.990591\n",
            "\n",
            " Time taken: 0 hours 3 minutes and 29.96 seconds.\n",
            "\n",
            " Fold 01 class insult AUC: 0.982115\n",
            "\n",
            " Fold 02 class insult AUC: 0.983051\n",
            "\n",
            " Fold 03 class insult AUC: 0.982665\n",
            "\n",
            " Average class insult AUC:\t0.982610\n",
            " Out-of-fold class insult AUC:\t0.982604\n",
            "\n",
            " Time taken: 0 hours 3 minutes and 25.53 seconds.\n",
            "\n",
            " Fold 01 class identity_hate AUC: 0.982633\n",
            "\n",
            " Fold 02 class identity_hate AUC: 0.983477\n",
            "\n",
            " Fold 03 class identity_hate AUC: 0.980580\n",
            "\n",
            " Average class identity_hate AUC:\t0.982230\n",
            " Out-of-fold class identity_hate AUC:\t0.982209\n",
            "\n",
            " Time taken: 0 hours 2 minutes and 25.57 seconds.\n",
            "\n",
            " Overall AUC:\t0.985396\n",
            "\n",
            " Time taken: 0 hours 19 minutes and 2.13 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}